# Impact to Organization


## Minimum Qualifications
### **Senior Scientist** - Bachelor’s degree in relevant discipline plus fifteen or more years of experience, or equivalent.
### **Distinguished** - Bachelor’s degree in relevant discipline plus twelve or more years of experience, or equivalent.
I exceed the Distinguished Member of Technical Staff education/experience requirements.  I have a Master's degree in Computer Science, as well as sixteen years experience in High Performance Computing.  I spent thirteen years at Los Alamos National Laboratory in the HPC Division, where I was hired on after a year as a Post-Master's Intern, focusing on Software Quality Assurance and HPC testing. From that internship, I joined a production support team, where I balanced many FOUS responsibilities, ranging from software support for HPC scientific software, installation management and modernization upon HPC systems at LANL, usability, functionality, performance and acceptance testing, many project leadership roles, team creation, technical and team leadership roles, and personnel management for the Programming & Runtime Environments Team.


[Leadership]
**PRETeam**... 
I founded the Programming & Runtime Environments Team at LANL around 2011, and led that team until I transitioned to Sandia at the end of fiscal year 2023.  

**Mustang Supercomputer Project**:  *Gary Grider, LANL Division Leader*, *Jeff Johnson, LANL HPC-3 Group Leader*:  Very unstable supercomputer in production that was almost unusable due to anomalous behavior and unreliability: At LANL, I was often given the responsibility of leading concerted testing efforts, including a large project to systematically root-cause performance and stability issues on a SuperMicro TLCC2-like system, that featured AMD Magny-cours processors, and QLogic HSN. Problems on this system ranged from fundamental flaws on the motherboard design, to the cabinet configuration, as well as NUMA configurations and datacenter cooling problems. I formed a small "tiger" team ranging from performance experts, systems architects, network engineers, mechanical engineers, and electrical engineers, and we analyzed the totality of the system, using benchmarks and LDMS data to support performance anomalies until we were able to stabilize the system for improved performance and stability. The outcomes of this project fed into future acquisitions and data center design decisions. 

**Chicoma Acceptance Testing Project**: *Carolyn Connor, LANL Institutional Computing Interim Program Manager*, *Jim Lujan, ASC Program Platform Director*: Historically, LANL HPC kept the acceptance testing of new procurements in the scope of the HPC Design Team, which was problematic because there was poor communication among groups, integration support suffered from gaps in RFPs, and there was no seemless process where the new HPC requirements reflected lessons learned and knowledge from one system to the other.  I partnered with the HPC Design Acceptance Tester to correct this posture, and he and I worked together to create a testing capability that spanned the entire lifecycle of a HPC system.  This encompassed using system monitoring and testing data to drive requirements for future procurements, a single suite of acceptance tests, that evolved as we gathered reproducers of new feature functionality, performance testing approaches, and mission applications and proxies.  Through this procedural transformation in LANL's HPC Division, we improved the ability of teams to acceptance test a new system, then carry forth those testing artifacts through the integration phase of deployment, into a production mode.  We conducted regular functional, reliability, performance, stability, and regression tests of the system and system software, and baselined the system to have a viable means to readily identify performance degradation of components. This cability hardened, and with the CARES act of 2020, where we received special funding to stand up a system to target the Coronavirus epidemic, we proved our process by taking over acceptance testing.  This project was a success, and set the new standard for acceptance testing, which I carried into the ATS-3 Crossroads supercomputer acceptance testing project, which I led on behalf of the DOE NNSA laboratories.

Over my career, I specialized in major supercomputer installations' software, testing, and customer support tickets ranging from Roadrunner to Crossroads- approximately 25 supercomputers scaling to 2.2M processing elements, supporting the NNSA Stockpile Stewardship Program and Nuclear Deterence initative.  I departed LANL the day we accepted the Crossroads supercomputer installation from HPE, and continued to ensure that LANL was able to sustain the capability by guiding my replacement while taking a R&D S&E CS Principal position in HPC Development at Sandia.  I continued leading the Crossroads integration in my capacity at Sandia, guiding the production support team through mitigation of filesystem stability problems, assisting management and my successor with navigating procurement management for production licensed software management tasks, and supporthing the team in supporting the ParaView user interface, VNC user interface, and kernel module packaging tasks.  That spirit of cross-laboratory leadership continues in my new roles at Sandia, assisting in leading package management, installation, and configuration management of the LDMS (Lightweight Distributed Metric Service), and serving as an SME to LANL staff on an ad hoc basis regarding Programming Environment support. 
Leadership: Crossroads Acceptance Testing

Leadership: 
## Nature of Work
- My technical expertise impact spans multiple Orgs, Divisions, and Laboratories, and disciplines, as I am sought out for advice and technical input in the areas of Programming Environments in HPC, Performance Analysis Tools, Accelerator Performance APIs, Package Management approaches (SPACK, RPM, Debian), Build Utilities (git, CMake, GNUMake), Container Orchestration Tools (Charliecloud, Podman, Gitlab CI, Github Actions, Docker), Cloud Infrastructure, HPC Performance Testing and Benchmarking, Monitoring system support, System and Application Analysis tools and techniques, Visualization software, and HPC workload analysis and debugging. 
## Creativity
- I created a new approach to packaging our LDMS product, integrating RPM and Debian packaging into a CICD workflow that builds LDMS in Linux containers, then deploys LDMS as a system service running in those containers for ultimately transforming our build-test-deploy capability, with the goal target of deploying a Terraform cluster using these containers to do automated functional, scale, deployment, and resiliency testing of our flagship product. 
## Operational Latitude
- I helped to interview and hire-on new four matrixed employees to file roles on the 9328 Operational Analytics team, and defined initial workplans to get them ramped up and productive in a short time frame.  Those projects have been handed over to the new staff members, who are now contributing to the projects independently after initial training and mentoring.
- I has been pulled into roles representing Sandia in the DOE NNSA ASC Software Strategy project, serving on the Cloud Computing and Programming Environment sub-working groups.  The latter is building on a Tri-laboratory launched in 2023, the ASC-PE project, that I initiated during her last year at LANL prior to joining Sandia, however the tenets and plans are continuing building on her original vision and direction, and I’s still pulled in as an SME in this project.
## Responsibility for Contacts
- I was invited to and participated in a 9321 Solutions Architect Experienced hiring committee, where I assisted in evaluating technical capabilities of candidates, and helped the committee reach a final decision on selecting a new hire.
- I regularly serves on review committees for the DOE (L2 IC ElCap 2025 for SPARC and EMPIRE, OLCF-6 Technical Design Review) and for technical conferences (IPDPS, SC, LDMSCON), serving as Vice-Chair for the SC’25 Reproducibility Committee, and Chair for SC’26 Reproducibility Committee AD/AE, and a member of the LDMSCON steering committee.
- I mentors staff on the team, bringing new ideas and technical approaches to supplying software for LDMS streams deamon installations on our systems.  I enabled a module file framework that enabled templated module files that dynamically adjust paths to library installation depending on user’s loaded environment.  I led her colleagues on setting up automation via Gitlab CICD to enable continuous testing and deployment of those plugin libraries to ensure usability, availability, and reliability of the software, while educating staff on new tools and approaches.
- I worked with LANL’s student intern to effectively install, configure, and deploy LDMS monitoring across their datacenter.  I continues to work with LANL to ensure that they can continue to maintain their monitoring capability amid system upgrades, new system acquisitions, and staff changes.
- I works with Industry to build specialized packages for supporting switch hardware running on HPE’s Slingshot Switches.  I works with OGC and actively supports LDMS as a product, fixing issues I encounters and building new capability.  I am trained and assigned as an SDR on the OGC contract, and ensures that the vendor is on target, the deliverables are met, and works through technical obstacles when encountered.
## Sphere of Influence / Potential Impact on Organizations
### **LDMS PROJECT**
- I’ve taken lead over the Operational Analytics team’s production support project, and achieved uniformity in our LDMS installations across the datacenter.  Employing modern tooling, and applying creativity, I designed a CICD LDMS Container build, test, and deploy pipeline that targets future requirements while satisfying present requirements.  I organize work and priority to ensure that we progress towards continued improvement and hardened installations for the system configurations, deployment model, and installation management processes for deployments on production systems.  Part of this project include taking over deployment management from the 9327 CapViz team integrators, and integrating LDMS receipes into the production image management service.  In this capacity, I transitioned the monitoring system setup from what was manually built, installed, and configured services setup to one that uses Trilab Operating System Stack (TOSS) supplied packages, conformant with tri-laboratory standards.  I work actively with the TOSS packaging community to ensure the LDMS ecosystem packages are built to satisfy Sandia needs, and that Sandia has voice over version control, build configurations, and packaging requirements on the software we have investments, both monetary and technical.  Doing this, the lab spends less budget on the support process of LDMS, and the team focuses now on process improvement and customer support.
### **ATS Programming Environment Specialist**
- My expertise is utilized in the standup and support of the ATS-3 Sandia Crossroads Application Development Testbed, "Tachi", the ATS-4 Sandia supercomputer - ranked 20th on the TOP500 chart, "El Dorado", a small version of El Capitan (the world's fastest supercomputer), where I’ve assisted the SPARC, SIERRA, and QMCPACK code teams to get their applications running, debugged advanced issues, and lead the effort to establish a software support posture to ensure that those systems are close proxies to their parent systems at LANL and LLNL, respectively.  These are examples of how I have made positive impact to the success of the Advanced Simulation & Computing program, ensuring that Sandia codes run correctly and performantly on advanced technology platforms, and are able to accomplish mission scientific pursuits.



 

## HPC Systems
[Roadrunner](https://top500.org/system/176027) was located at Los Alamos and built by IBM. This was the first supercomputer to break the once-elusive petaFLOP barrier, meaning that Roadrunner was capable of one quadrillion, or a thousand trillion, floating point operations per second. It helped close a difficult, long-standing gap in understanding of energy flow in a weapon and its relation to weapon yield. A power-efficient hybrid computer, Roadrunner held the No. 1 spot on TOP500 from June 2008-2009, as well as the No. 3 spot on the Green500 list of energy-efficient supercomputers.

[TLCC-2 - Mustang](https://top500.org/system/177456/), The LANL Mustang supercomputer was an Appro Xtreme-X system deployed at Los Alamos National Laboratory (LANL) around 2011, featuring AMD Opteron processors and over 37,000 cores, providing hundreds of teraflops of performance for unclassified research and significantly boosting LANL's high-performance computing (HPC) capabilities before being succeeded by newer systems like Trinity and the upcoming Mission and Vision supercomputers

[ACES - Cielo](https://top500.org/system/176951/) was located at Los Alamos and was built by Cray. With a speed of 1.4 petaFLOPS, Cielo was the first supercomputer to run routine 3-D weapons simulations. It was used for engineering design and assessment in support of critical B61-12 Life Extension Program (LEP) deliverables: for example, captive carry simulations of the F-35 informed design decisions.

[ATS-1 - Trinity](https://top500.org/system/178610/) is located at Los Alamos and was built by Cray. With a peak performance of 41.5 petaFLOPS, it presents an 8x improvement over Cielo in fidelity, physics, and performance capabilities. As the most recent supercomputing addition to come fully online, Trinity has provided enhanced computational capability and greater exactness for geometry and physics simulations.

[Chicoma](https://share.google/WFfGcEiLERwPE14Bx), an HPE Cray EX system networked with HPE Slingshot, boasts 688 nodes and more than 300TB of memory. 560 of the nodes are equipped with dual AMD Epyc 7H12 CPUs, while the remaining 128 are equipped with individual AMD Epyc 7713 CPUs and quadruple Nvidia A100 GPUs. HPCwire estimates that Chicoma offers more than eight petaflops of peak computing power. Chicoma is the first system specifically designed to offer multiple architectures within a single large platform. Chicoma was acquired as part of the 2020 CARES Act to support research related to the COVID-19 pandemic, focusing on molecular dynamics, epidemiological modeling, bioinformatics, and chromosome/RNA simulations.

[ATS-3 - Crossroads](https://top500.org/system/180178/), the third Advanced Technology (AT) system in the Advanced Simulation and Computing (ASC) Program, will be critical to the success of the National Nuclear Security Administration (NNSA) Stockpile Stewardship Program (SSP). Crossroads supports the highest fidelity of next-generation weapons simulations and meet NNSA Defense Programs’ mission needs. In addition, Crossroads will explore and exploit new technologies with a focus on improving efficiency in three key areas: application performance, workflow, and application development.

[ATS-4 - El Capitan](https://top500.org/system/180307/), an HPE Cray EX255a, AMD 4th Gen EPYC 24C 1.8GHz, AMD Instinct MI300A, Slingshot-11, TOSS.  Is the NNSA's first exascale supercomputer. An exascale supercomputer can calculate at least one quintillion (1,000,000,000,000,000,000+) double precision (64-bit) operations per second (1 exaflop). Deployed in 2024, El Capitan is ranked as the world’s most powerful supercomputer, capable of performing more than 2.79 exaflops per second.
